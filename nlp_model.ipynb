{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0W6r42eQIY530QvAgQsSz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tigerrex9/Senior-Project/blob/main/nlp_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "9wUqBtCiDKCU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W5OuviDpsV7b"
      },
      "outputs": [],
      "source": [
        "### Import statements\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for merging new history objects with older ones\n",
        "def append_history(losses, val_losses, accuracy, val_accuracy, history):\n",
        "    losses = losses + history.history[\"loss\"]\n",
        "    val_losses = val_losses + history.history[\"val_loss\"]\n",
        "    accuracy = accuracy + history.history[\"binary_accuracy\"]\n",
        "    val_accuracy = val_accuracy + history.history[\"val_binary_accuracy\"]\n",
        "    return losses, val_losses, accuracy, val_accuracy\n",
        "\n",
        "# Plotter function\n",
        "def plot_history(losses, val_losses, accuracies, val_accuracies):\n",
        "    plt.plot(losses)\n",
        "    plt.plot(val_losses)\n",
        "    plt.legend([\"train_loss\", \"val_loss\"])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(accuracies)\n",
        "    plt.plot(val_accuracies)\n",
        "    plt.legend([\"train_accuracy\", \"val_accuracy\"])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kF18WyqikMFh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tfds.load(\n",
        "    \"imdb_reviews\",\n",
        "    split=\"train + test\",\n",
        "    as_supervised=True,\n",
        "    batch_size=-1,\n",
        "    shuffle_files=False,\n",
        ")\n",
        "reviews, labels = tfds.as_numpy(dataset)"
      ],
      "metadata": {
        "id": "MYQKjy1BxPb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e262550-17eb-4ab1-de59-140e97234290"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total examples: 50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reviews.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbvGqdf6Gg1X",
        "outputId": "6b98d986-f90f-4a5c-f1f9-0299469046fc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_split = 2500\n",
        "test_split = 2500\n",
        "train_split = 7500\n",
        "\n",
        "# Separating the negative and positive samples for manual stratification\n",
        "x_positives, y_positives = reviews[labels == 1], labels[labels == 1]\n",
        "x_negatives, y_negatives = reviews[labels == 0], labels[labels == 0]\n",
        "\n",
        "# Creating training, validation and testing splits\n",
        "x_val, y_val = (\n",
        "    tf.concat((x_positives[:val_split], x_negatives[:val_split]), 0),\n",
        "    tf.concat((y_positives[:val_split], y_negatives[:val_split]), 0),\n",
        ")\n",
        "x_test, y_test = (\n",
        "    tf.concat(\n",
        "        (\n",
        "            x_positives[val_split : val_split + test_split],\n",
        "            x_negatives[val_split : val_split + test_split],\n",
        "        ),\n",
        "        0,\n",
        "    ),\n",
        "    tf.concat(\n",
        "        (\n",
        "            y_positives[val_split : val_split + test_split],\n",
        "            y_negatives[val_split : val_split + test_split],\n",
        "        ),\n",
        "        0,\n",
        "    ),\n",
        ")\n",
        "x_train, y_train = (\n",
        "    tf.concat(\n",
        "        (\n",
        "            x_positives[val_split + test_split : val_split + test_split + train_split],\n",
        "            x_negatives[val_split + test_split : val_split + test_split + train_split],\n",
        "        ),\n",
        "        0,\n",
        "    ),\n",
        "    tf.concat(\n",
        "        (\n",
        "            y_positives[val_split + test_split : val_split + test_split + train_split],\n",
        "            y_negatives[val_split + test_split : val_split + test_split + train_split],\n",
        "        ),\n",
        "        0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Remaining pool of samples are stored separately. These are only labeled as and when required\n",
        "x_pool_positives, y_pool_positives = (\n",
        "    x_positives[val_split + test_split + train_split :],\n",
        "    y_positives[val_split + test_split + train_split :],\n",
        ")\n",
        "x_pool_negatives, y_pool_negatives = (\n",
        "    x_negatives[val_split + test_split + train_split :],\n",
        "    y_negatives[val_split + test_split + train_split :],\n",
        ")\n",
        "\n",
        "# Creating TF Datasets for faster prefetching and parallelization\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "\n",
        "pool_negatives = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_pool_negatives, y_pool_negatives)\n",
        ")\n",
        "pool_positives = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_pool_positives, y_pool_positives)\n",
        ")\n",
        "\n",
        "print(f\"Initial training set size: {len(train_dataset)}\")\n",
        "print(f\"Validation set size: {len(val_dataset)}\")\n",
        "print(f\"Testing set size: {len(test_dataset)}\")\n",
        "print(f\"Unlabeled negative pool: {len(pool_negatives)}\")\n",
        "print(f\"Unlabeled positive pool: {len(pool_positives)}\")\n"
      ],
      "metadata": {
        "id": "fKwWVbEqM8Xq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
        "    )\n",
        "\n",
        "\n",
        "vectorizer = layers.TextVectorization(\n",
        "    3000, standardize=custom_standardization, output_sequence_length=150\n",
        ")\n",
        "# Adapting the dataset\n",
        "vectorizer.adapt(\n",
        "    train_dataset.map(lambda x, y: x, num_parallel_calls=tf.data.AUTOTUNE).batch(256)\n",
        ")\n",
        "\n",
        "\n",
        "def vectorize_text(text, label):\n",
        "    text = vectorizer(text)\n",
        "    return text, label\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.map(\n",
        "    vectorize_text, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "pool_negatives = pool_negatives.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "pool_positives = pool_positives.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "val_dataset = val_dataset.batch(256).map(\n",
        "    vectorize_text, num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "test_dataset = test_dataset.batch(256).map(\n",
        "    vectorize_text, num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "EzG9hykQOddK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "jCMJfPbYDVxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Make Model\n",
        "def create_model():\n",
        "    model = keras.models.Sequential(\n",
        "        [\n",
        "            layers.Input(shape=(150,)),\n",
        "            layers.Embedding(input_dim=3000, output_dim=128),\n",
        "            layers.Bidirectional(layers.LSTM(32, return_sequences=True)),\n",
        "            layers.GlobalMaxPool1D(),\n",
        "            layers.Dense(20, activation=\"relu\"),\n",
        "            layers.Dropout(0.5),\n",
        "            layers.Dense(1, activation=\"sigmoid\"),\n",
        "        ]\n",
        "    )\n",
        "    model.summary()\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "bIcNP9XXDYBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_full_model(full_train_dataset, val_dataset, test_dataset):\n",
        "    model = create_model()\n",
        "    model.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=\"rmsprop\",\n",
        "        metrics=[\n",
        "            keras.metrics.BinaryAccuracy(),\n",
        "            keras.metrics.FalseNegatives(),\n",
        "            keras.metrics.FalsePositives(),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # We will save the best model at every epoch and load the best one for evaluation on the test set\n",
        "    history = model.fit(\n",
        "        full_train_dataset.batch(256),\n",
        "        epochs=20,\n",
        "        validation_data=val_dataset,\n",
        "        callbacks=[\n",
        "            keras.callbacks.EarlyStopping(patience=4, verbose=1),\n",
        "            keras.callbacks.ModelCheckpoint(\n",
        "                \"FullModelCheckpoint.h5\", verbose=1, save_best_only=True\n",
        "            ),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Plot history\n",
        "    plot_history(\n",
        "        history.history[\"loss\"],\n",
        "        history.history[\"val_loss\"],\n",
        "        history.history[\"binary_accuracy\"],\n",
        "        history.history[\"val_binary_accuracy\"],\n",
        "    )\n",
        "\n",
        "    # Loading the best checkpoint\n",
        "    model = keras.models.load_model(\"FullModelCheckpoint.h5\")\n",
        "\n",
        "    print(\"-\" * 100)\n",
        "    print(\n",
        "        \"Test set evaluation: \",\n",
        "        model.evaluate(test_dataset, verbose=0, return_dict=True),\n",
        "    )\n",
        "    print(\"-\" * 100)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Sampling the full train dataset to train on\n",
        "full_train_dataset = (\n",
        "    train_dataset.concatenate(pool_positives)\n",
        "    .concatenate(pool_negatives)\n",
        "    .cache()\n",
        "    .shuffle(20000)\n",
        ")\n",
        "\n",
        "# Training the full model\n",
        "full_dataset_model = train_full_model(full_train_dataset, val_dataset, test_dataset)\n"
      ],
      "metadata": {
        "id": "yFSVhPmYOv9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_active_learning_models(\n",
        "    train_dataset,\n",
        "    pool_negatives,\n",
        "    pool_positives,\n",
        "    val_dataset,\n",
        "    test_dataset,\n",
        "    num_iterations=3,\n",
        "    sampling_size=5000,\n",
        "):\n",
        "\n",
        "    # Creating lists for storing metrics\n",
        "    losses, val_losses, accuracies, val_accuracies = [], [], [], []\n",
        "\n",
        "    model = create_model()\n",
        "    # We will monitor the false positives and false negatives predicted by our model\n",
        "    # These will decide the subsequent sampling ratio for every Active Learning loop\n",
        "    model.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=\"rmsprop\",\n",
        "        metrics=[\n",
        "            keras.metrics.BinaryAccuracy(),\n",
        "            keras.metrics.FalseNegatives(),\n",
        "            keras.metrics.FalsePositives(),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Defining checkpoints.\n",
        "    # The checkpoint callback is reused throughout the training since it only saves the best overall model.\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        \"AL_Model.h5\", save_best_only=True, verbose=1\n",
        "    )\n",
        "    # Here, patience is set to 4. This can be set higher if desired.\n",
        "    early_stopping = keras.callbacks.EarlyStopping(patience=4, verbose=1)\n",
        "\n",
        "    print(f\"Starting to train with {len(train_dataset)} samples\")\n",
        "    # Initial fit with a small subset of the training set\n",
        "    history = model.fit(\n",
        "        train_dataset.cache().shuffle(20000).batch(256),\n",
        "        epochs=20,\n",
        "        validation_data=val_dataset,\n",
        "        callbacks=[checkpoint, early_stopping],\n",
        "    )\n",
        "\n",
        "    # Appending history\n",
        "    losses, val_losses, accuracies, val_accuracies = append_history(\n",
        "        losses, val_losses, accuracies, val_accuracies, history\n",
        "    )\n",
        "\n",
        "    for iteration in range(num_iterations):\n",
        "        # Getting predictions from previously trained model\n",
        "        predictions = model.predict(test_dataset)\n",
        "\n",
        "        # Generating labels from the output probabilities\n",
        "        rounded = tf.where(tf.greater(predictions, 0.5), 1, 0)\n",
        "\n",
        "        # Evaluating the number of zeros and ones incorrrectly classified\n",
        "        _, _, false_negatives, false_positives = model.evaluate(test_dataset, verbose=0)\n",
        "\n",
        "        print(\"-\" * 100)\n",
        "        print(\n",
        "            f\"Number of zeros incorrectly classified: {false_negatives}, Number of ones incorrectly classified: {false_positives}\"\n",
        "        )\n",
        "\n",
        "        # This technique of Active Learning demonstrates ratio based sampling where\n",
        "        # Number of ones/zeros to sample = Number of ones/zeros incorrectly classified / Total incorrectly classified\n",
        "        if false_negatives != 0 and false_positives != 0:\n",
        "            total = false_negatives + false_positives\n",
        "            sample_ratio_ones, sample_ratio_zeros = (\n",
        "                false_positives / total,\n",
        "                false_negatives / total,\n",
        "            )\n",
        "        # In the case where all samples are correctly predicted, we can sample both classes equally\n",
        "        else:\n",
        "            sample_ratio_ones, sample_ratio_zeros = 0.5, 0.5\n",
        "\n",
        "        print(\n",
        "            f\"Sample ratio for positives: {sample_ratio_ones}, Sample ratio for negatives:{sample_ratio_zeros}\"\n",
        "        )\n",
        "\n",
        "        # Sample the required number of ones and zeros\n",
        "        sampled_dataset = pool_negatives.take(\n",
        "            int(sample_ratio_zeros * sampling_size)\n",
        "        ).concatenate(pool_positives.take(int(sample_ratio_ones * sampling_size)))\n",
        "\n",
        "        # Skip the sampled data points to avoid repetition of sample\n",
        "        pool_negatives = pool_negatives.skip(int(sample_ratio_zeros * sampling_size))\n",
        "        pool_positives = pool_positives.skip(int(sample_ratio_ones * sampling_size))\n",
        "\n",
        "        # Concatenating the train_dataset with the sampled_dataset\n",
        "        train_dataset = train_dataset.concatenate(sampled_dataset).prefetch(\n",
        "            tf.data.AUTOTUNE\n",
        "        )\n",
        "\n",
        "        print(f\"Starting training with {len(train_dataset)} samples\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        # We recompile the model to reset the optimizer states and retrain the model\n",
        "        model.compile(\n",
        "            loss=\"binary_crossentropy\",\n",
        "            optimizer=\"rmsprop\",\n",
        "            metrics=[\n",
        "                keras.metrics.BinaryAccuracy(),\n",
        "                keras.metrics.FalseNegatives(),\n",
        "                keras.metrics.FalsePositives(),\n",
        "            ],\n",
        "        )\n",
        "        history = model.fit(\n",
        "            train_dataset.cache().shuffle(20000).batch(256),\n",
        "            validation_data=val_dataset,\n",
        "            epochs=20,\n",
        "            callbacks=[\n",
        "                checkpoint,\n",
        "                keras.callbacks.EarlyStopping(patience=4, verbose=1),\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        # Appending the history\n",
        "        losses, val_losses, accuracies, val_accuracies = append_history(\n",
        "            losses, val_losses, accuracies, val_accuracies, history\n",
        "        )\n",
        "\n",
        "        # Loading the best model from this training loop\n",
        "        model = keras.models.load_model(\"AL_Model.h5\")\n",
        "\n",
        "    # Plotting the overall history and evaluating the final model\n",
        "    plot_history(losses, val_losses, accuracies, val_accuracies)\n",
        "    print(\"-\" * 100)\n",
        "    print(\n",
        "        \"Test set evaluation: \",\n",
        "        model.evaluate(test_dataset, verbose=0, return_dict=True),\n",
        "    )\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "active_learning_model = train_active_learning_models(\n",
        "    train_dataset, pool_negatives, pool_positives, val_dataset, test_dataset\n",
        ")\n"
      ],
      "metadata": {
        "id": "3LUFr353PECx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance"
      ],
      "metadata": {
        "id": "SdgpNckTDYV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Run\n",
        "plot_history(losses, val_losses, accuracies, val_accuracies)"
      ],
      "metadata": {
        "id": "oW2T4ze2Da8T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}